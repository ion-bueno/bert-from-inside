# Inside a NLP multitasking neural network. BERT, one model to rule them all
NLP is an well known topic inside AI field. A wide range of applications are collected
into this area, from text classification as sentiment analysis to text generation like translation.
With the potential technological growth, these tools play a critical role in daily
business automating certain tasks. In spite of its benefit, NLP has been a leveraged field
into AI during many years. Training times of RNNs and the difficulty to capture long-term
dependencies were an extended problem. However, the emergence of the transformer architecture
with its pure attention mechanisms represented a critical change in the development
of models. This results in the publication of BERT, overcoming all current models
and getting state-of-the-art results. This model was pre-trained and it is defined for how
easy and fast can be fine-tuned for most NLP tasks. In order to show this process, two
applications are developed for sentiment analysis and question answering problems. In
both cases, training time was really short compared with an RNN, obtaining an outstanding
performance in test set. BERT implied the origin of pre-training language models,
which currently is the dominant trend in NLP area. In addition, it resulted in the solution
for long training times and a huge amount of data in order to have a reliable model for a
specific task.
